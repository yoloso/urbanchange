# 06_create_panel.py
# Generates a panel for a specific location and time period
#
# Usage: Run the following command in terminal (modified to your neighborhood of choice)
#   python 06_create_panel.py
#
# Data inputs:
#   - CSV file including one row per detected object instance (generated
#     using 01_detect_segments.py on the selected neighborhoods)
#   - Segment dictionary for the selected neighborhood
#   - Image log for the selected neighborhood generated during the image
#   collection process
#   - Urban index generated by 04_indices_in_time.py
#
# Outputs:
#   - CSV file

import datetime
from datetime import date
import json
import numpy as np
import os
import pandas as pd

from DataScripts.read_files import load_segment_dict
from DataScripts.read_files import prep_object_vectors_with_dates
#TODO PENDING generate urban index excluding tents

# Parameters
SEGMENT_DICTIONARY_FILE = os.path.join(
    'Data', 'ProcessedData', 'SFStreetView', 'segment_dictionary_SFTenderloin.json')
IMAGES_DIR = os.path.join(
    'Data', 'ProcessedData', 'SFStreetView', 'Res_640', 'SFTenderloin_full_2009_2021')
OBJECT_VECTORS_DIR = os.path.join(
    'Outputs', 'Detection', 'Res_640', 'SFTenderloin_full_2009_2021')
URBAN_INDEX = os.path.join(
    'Outputs', 'Urban_quality', 'Res_640', 'SFTenderloin_full_2009_2021',
    'indices_count_pano_adjustment_50.csv')
OUTPUT_DIR = os.path.join(
    'Data', 'ProcessedData', 'UseCases', 'SFTenderloin'
)
SELECTED_INDEX = 'weighted_sum_log'
PERIOD = {'start': date(2009, 1, 1), 'end': date(2021, 7, 31)}
CONFIDENCE_LEVEL = 50
# Define treatment (number of quarters)
TREATMENT = 4
# Define urban change


# Helper functions
def aggregate_tent_presence(x):
    if x.empty:
        return None
    else:
        if np.isnan(x).sum() == len(x):
            return np.nan
        else:
            tent_sum = np.nansum(x)
            tent_indicator = 1 if tent_sum > 0 else 0
            return tent_indicator


def aggregate_urban_index(x):
    if x.empty:
        return None
    else:
        if np.isnan(x).sum() == len(x):
            return np.nan
        else:
            return np.nanmean(x)


def generate_treatment(row):
    pass # TODO


# Load files
segment_dictionary = load_segment_dict(SEGMENT_DICTIONARY_FILE)
object_vectors = prep_object_vectors_with_dates(OBJECT_VECTORS_DIR, IMAGES_DIR)

# Load selected urban index
try:
    with open(URBAN_INDEX, 'r') as file:
        urban_index = pd.read_csv(file)
except FileNotFoundError:
    raise Exception('[ERROR] Urban index file not found.')

try:
    urban_index['index'] = urban_index[SELECTED_INDEX]
    urban_index = urban_index[['segment_id', 'segment_date', 'index', 'tent']]
except KeyError:
    raise Exception('[ERROR] Selected index is not found in urban index file.')

# Fix dates
object_vectors['segment_date'] = pd.to_datetime(object_vectors['img_date'])
urban_index['segment_date'] = pd.to_datetime(urban_index['segment_date'])

object_vectors['segment_date'] = object_vectors['segment_date'].apply(
    lambda x: x.date())
urban_index['segment_date'] = urban_index['segment_date'].apply(
    lambda x: x.date())

# Get tent instances
tent_vectors = object_vectors[object_vectors['class'] == 'tent'].copy()
tent_vectors = tent_vectors[tent_vectors['confidence'] >= CONFIDENCE_LEVEL / 100]
tent_vectors = tent_vectors.groupby(['segment_id', 'segment_date']).size().\
    reset_index(name='count')
tent_vectors['tent_indicator'] = 1

# Generate base panel
months = [ts.date() for ts in pd.date_range(
    PERIOD['start'], PERIOD['end'], freq='MS')]
month_df = pd.DataFrame({'segment_date': months})

hashed_segments = [
    json.loads(seg['segment_id']) for seg in segment_dictionary.values()]
hashed_segments = ['{}-{}'.format(seg_id[0], seg_id[1]) for seg_id in hashed_segments]
segment_df = pd.DataFrame({'segment_id': hashed_segments})

# Add tent indicator to base panel
base_panel = segment_df.merge(month_df, how='cross')
base_panel = base_panel.merge(
    tent_vectors[['segment_id', 'segment_date', 'tent_indicator']],
    how='left', on=['segment_id', 'segment_date'], validate='one_to_one')

# Add urban index to base panel
base_panel = base_panel.merge(
    urban_index[['segment_id', 'segment_date', 'index', 'tent']],
    how='left', on=['segment_id', 'segment_date'], validate='one_to_one')

# Modify zeros in base panel: We need to identify cases where zero tents
# were detected, as these are currently fake "nan"s
base_panel['tent_indicator'] = base_panel.apply(
    lambda row: 0 if pd.isnull(row['tent_indicator']) and pd.notnull(row['index']) else row['tent_indicator'],
    axis=1
)

# Aggregate observations on a quarterly basis
quarterly_panel = base_panel[
    ['segment_id', 'segment_date', 'tent_indicator', 'index']].copy()
quarterly_panel['quarter'] = pd.PeriodIndex(
    quarterly_panel['segment_date'], freq='Q')

quarterly_panel = quarterly_panel.groupby(['segment_id', 'quarter']).\
    agg({'tent_indicator': aggregate_tent_presence,
         'index': aggregate_urban_index}).reset_index()

# Generate final panel
final_panel = quarterly_panel.copy()

# Generate lagged columns
shifted_panels = [final_panel]
for lag in range(1, TREATMENT + 1):
    # Change column names
    lagged_panel = final_panel[['tent_indicator', 'index']].copy()
    lagged_panel.rename(
        columns={'tent_indicator': 'tent_indicator{}'.format(lag),
                 'index': 'index_indicator{}'.format(lag)}, inplace=True)
    lagged_panel = lagged_panel.shift(lag)
    shifted_panels.append(lagged_panel)

final_panel = pd.concat(shifted_panels, axis=1)
final_panel.to_csv(os.path.join(OUTPUT_DIR, 'lagged_panel_quarters.csv'), index=False)

# Generate treatment column
final_panel['treatment'] = final_panel.apply(lambda row: generate_treatment(row), axis=1)
